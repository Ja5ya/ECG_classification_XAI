{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load your data\n",
    "\n",
    "Before finetuning a pretrained model of the experiments we provide in our repository (or precomputed and provided [here](https://datacloud.hhi.fraunhofer.de/nextcloud/s/NCjYws3mamLrkKq)), first load your custom 100 Hz sampled 12-lead ECG signal data `X` of shape `[N,L,12]` in Millivolts (mV) and multi-hot encoded labels `y` of shape `[N,C]` as numpy arrays, where `C` is the number of classes and `N` the number of total samples in this dataset. Although PTB-XL comes with fixed `L=1000` (i,e. 10 seconds), it is not required to be fixed, **BUT** the shortest sample must be longer than `input_size` of the specific model (e.g. 2.5 seconds for our fastai-models).\n",
    "\n",
    "For proper tinetuning split your data into four numpy arrays: `X_train`,`y_train`,`X_val` and `y_val`\n",
    "\n",
    "### Example: finetune model trained on all (71) on superdiagnostic (5)\n",
    "Below we provide an example for loading [PTB-XL](https://physionet.org/content/ptb-xl/1.0.1/) aggregated at the `superdiagnostic` level, where we use the provided folds for train-validation-split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19225, 1000, 12), (19225, 5), (2158, 1000, 12), (2158, 5))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ecg_ptbxl_benchmarking.code.utils import utils\n",
    "\n",
    "sampling_frequency=100\n",
    "datafolder='/global/D1/homes/jayao/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.2/ptbxl/'\n",
    "task='superdiagnostic'\n",
    "outputfolder='/global/D1/homes/jayao/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.2/output/'\n",
    "\n",
    "# Load PTB-XL data\n",
    "data, raw_labels = utils.load_dataset(datafolder, sampling_frequency)\n",
    "# Preprocess label data\n",
    "labels = utils.compute_label_aggregations(raw_labels, datafolder, task)\n",
    "# Select relevant data and convert to one-hot\n",
    "data, labels, Y, _ = utils.select_data(data, labels, task, min_samples=0, outputfolder=outputfolder)\n",
    "\n",
    "# 1-9 for training \n",
    "X_train = data[labels.strat_fold < 10]\n",
    "y_train = Y[labels.strat_fold < 10]\n",
    "# 10 for validation\n",
    "X_val = data[labels.strat_fold == 10]\n",
    "y_val = Y[labels.strat_fold == 10]\n",
    "\n",
    "num_classes = 5         # <=== number of classes in the finetuning dataset\n",
    "input_shape = [1000,12] # <=== shape of samples, [None, 12] in case of different lengths\n",
    "\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train or download models\n",
    "There are two possibilities:\n",
    "   1. Run the experiments as described in README. Afterwards you find trained in models in `output/expX/models/`\n",
    "   2. Download the precomputed `output`-folder with all experiments and models from [here]((https://datacloud.hhi.fraunhofer.de/nextcloud/s/NCjYws3mamLrkKq))\n",
    "\n",
    "# Load pretrained model\n",
    "\n",
    "For loading a pretrained model:\n",
    "   1. specify `modelname` which can be seen in `code/configs/` (e.g. `modelname='fastai_xresnet1d101'`)\n",
    "   2. provide `experiment` to build the path `pretrainedfolder` (here: `exp0` refers to the experiment with `all` 71 SCP-statements)\n",
    "   \n",
    "This returns the pretrained model where the classification is replaced by a random initialized head with the same number of outputs as the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_fs: 100\n",
      "input_size: 100\n",
      "input_channels: 12\n",
      "chunkify_train: False\n",
      "chunkify_valid: True\n",
      "min_chunk_length: 100\n"
     ]
    }
   ],
   "source": [
    "from ecg_ptbxl_benchmarking.code.models.fastai_model import fastai_model\n",
    "\n",
    "experiment = 'test'\n",
    "modelname = 'fastai_xresnet1d101'\n",
    "pretrainedfolder = '/global/D1/homes/jayao/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.2/output/'+experiment+'/models/'+modelname+'/'\n",
    "mpath='/global/D1/homes/jayao/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.2/output/' # <=== path where the finetuned model will be stored\n",
    "n_classes_pretrained = 5 # <=== because we load the model from exp0, this should be fixed because this depends the experiment\n",
    "\n",
    "model = fastai_model(\n",
    "    modelname, \n",
    "    num_classes, \n",
    "    sampling_frequency, \n",
    "    mpath, \n",
    "    input_shape=input_shape, \n",
    "    pretrainedfolder=pretrainedfolder,\n",
    "    n_classes_pretrained=n_classes_pretrained, \n",
    "    pretrained=True,\n",
    "    epochs_finetuning=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data with pretrained Standardizer\n",
    "\n",
    "Since we standardize inputs to zero mean and unit variance, your custom data needs to be standardized with the respective mean and variance. This is also provided in the respective experiment folder `output/expX/data/standard_scaler.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# standard_scaler = pickle.load(open('../output/'+experiment+'/data/standard_scaler.pkl', \"rb\"))\n",
    "standard_scaler = pickle.load(open('/global/D1/homes/jayao/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.2/output/'+experiment+'/data/standard_scaler.pkl', \"rb\"))\n",
    "\n",
    "X_train = utils.apply_standardizer(X_train, standard_scaler)\n",
    "X_val = utils.apply_standardizer(X_val, standard_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune model\n",
    "\n",
    "Calling `model.fit` of a model with `pretrained=True` will perform finetuning as proposed in our work i.e. **gradual unfreezing and discriminative learning rates**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning...\n",
      "(19225, 2)\n",
      "(2158, 2)\n",
      "model: fastai_xresnet1d101\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_multi</th>\n",
       "      <th>balanced_accuracy_multi</th>\n",
       "      <th>precision_multi</th>\n",
       "      <th>recall_multi</th>\n",
       "      <th>specificity_multi</th>\n",
       "      <th>F1_multi</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='94' class='' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      62.67% [94/150 00:19&lt;00:11 2.1061]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_multi</th>\n",
       "      <th>balanced_accuracy_multi</th>\n",
       "      <th>precision_multi</th>\n",
       "      <th>recall_multi</th>\n",
       "      <th>specificity_multi</th>\n",
       "      <th>F1_multi</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.287521</td>\n",
       "      <td>0.309373</td>\n",
       "      <td>0.874845</td>\n",
       "      <td>0.815511</td>\n",
       "      <td>0.798704</td>\n",
       "      <td>0.690007</td>\n",
       "      <td>0.941015</td>\n",
       "      <td>0.736708</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.255112</td>\n",
       "      <td>0.293629</td>\n",
       "      <td>0.879752</td>\n",
       "      <td>0.827758</td>\n",
       "      <td>0.795500</td>\n",
       "      <td>0.718602</td>\n",
       "      <td>0.936915</td>\n",
       "      <td>0.751746</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_multi</th>\n",
       "      <th>balanced_accuracy_multi</th>\n",
       "      <th>precision_multi</th>\n",
       "      <th>recall_multi</th>\n",
       "      <th>specificity_multi</th>\n",
       "      <th>F1_multi</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='90' class='' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      60.00% [90/150 00:09&lt;00:06 0.7044]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_multi</th>\n",
       "      <th>balanced_accuracy_multi</th>\n",
       "      <th>precision_multi</th>\n",
       "      <th>recall_multi</th>\n",
       "      <th>specificity_multi</th>\n",
       "      <th>F1_multi</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.248278</td>\n",
       "      <td>0.296196</td>\n",
       "      <td>0.879591</td>\n",
       "      <td>0.828834</td>\n",
       "      <td>0.792657</td>\n",
       "      <td>0.722174</td>\n",
       "      <td>0.935495</td>\n",
       "      <td>0.752364</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.248823</td>\n",
       "      <td>0.297877</td>\n",
       "      <td>0.879567</td>\n",
       "      <td>0.827784</td>\n",
       "      <td>0.794571</td>\n",
       "      <td>0.718901</td>\n",
       "      <td>0.936669</td>\n",
       "      <td>0.751475</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2158, 2)\n",
      "(2158, 2)\n",
      "model: fastai_xresnet1d101\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregating predictions...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>macro_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.920827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   macro_auc\n",
       "0   0.920827"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_pred = model.predict(X_val)\n",
    "utils.evaluate_experiment(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19225, 1000, 12), (19225, 5), (2158, 1000, 12), (2158, 5))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ecg_ptbxl_benchmarking.code.utils import utils\n",
    "\n",
    "sampling_frequency=100\n",
    "datafolder='/global/D1/homes/jayao/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.2/ptbxl/'\n",
    "task='superdiagnostic'\n",
    "outputfolder='/global/D1/homes/jayao/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.2/output/'\n",
    "\n",
    "# Load PTB-XL data\n",
    "data, raw_labels = utils.load_dataset(datafolder, sampling_frequency)\n",
    "# Preprocess label data\n",
    "labels = utils.compute_label_aggregations(raw_labels, datafolder, task)\n",
    "# Select relevant data and convert to one-hot\n",
    "data, labels, Y, _ = utils.select_data(data, labels, task, min_samples=0, outputfolder=outputfolder)\n",
    "\n",
    "# 1-9 for training \n",
    "X_train = data[labels.strat_fold < 10]\n",
    "y_train = Y[labels.strat_fold < 10]\n",
    "# 10 for validation\n",
    "X_val = data[labels.strat_fold == 10]\n",
    "y_val = Y[labels.strat_fold == 10]\n",
    "\n",
    "num_classes = 5         # <=== number of classes in the finetuning dataset\n",
    "input_shape = [1000,12] # <=== shape of samples, [None, 12] in case of different lengths\n",
    "\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "#for losses_plot\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CyclicLR, ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "pretrainedfolder = '/global/D1/homes/jayao/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.2/output/models/'\n",
    "# your_model.pth\n",
    "mpath='/global/D1/homes/jayao/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.2/output/custom_test/models/' # <=== path where the finetuned model will be stored\n",
    "n_classes_pretrained = 5 # <=== because we load the model from exp0, this should be fixed because this depends the experiment\n",
    "\n",
    "# Define the Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        # ReLU activation function\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        # Shortcut connection (if needed)\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        shortcut = self.shortcut(x)\n",
    "        \n",
    "        out += shortcut  # Element-wise addition\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# Define the ResNet-like model\n",
    "class YourModel(nn.Module):\n",
    "    def __init__(self, categories):\n",
    "        super(YourModel, self).__init__()\n",
    "        # Define the layers for the model\n",
    "        self.conv1 = nn.Conv1d(12, 32, kernel_size=5, stride=1, padding=2)  # Adjust the input channels to match the input data\n",
    "        self.block1 = ResidualBlock(32, 32)\n",
    "        self.block2 = ResidualBlock(32, 32)\n",
    "        self.fc1 = nn.Linear(32000, 32)  # Adjust the input size based on your data\n",
    "        self.fc2 = nn.Linear(32, categories)  # Updated to match the desired number of output categories\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)  # Apply sigmoid activation\n",
    "        return x\n",
    "    \n",
    "    def accuracy_multi(self, inp, targ, thresh=0.5, sigmoid=True):\n",
    "        if sigmoid:\n",
    "            inp = inp.sigmoid()\n",
    "        return ((inp > thresh) == targ.bool()).float().mean()\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, path, categories=5):\n",
    "        model = cls(categories=categories)  # Create an instance of your model class\n",
    "        pretrained_file = os.path.join(path, 'your_model.pth')  # Specify the file name of the pre-trained model\n",
    "        model.load_state_dict(torch.load(pretrained_file))\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        return model\n",
    "\n",
    "        \n",
    "    def fit(self, X_train, y_train, X_val, y_val, sigmoid= True, model_save_path=None):\n",
    "        # Convert data to the appropriate data type (e.g., torch.float32)\n",
    "        X_train = torch.FloatTensor(X_train)\n",
    "        y_train = torch.FloatTensor(y_train)\n",
    "        X_val = torch.FloatTensor(X_val)\n",
    "        y_val = torch.FloatTensor(y_val)\n",
    "\n",
    "        X_train = X_train.permute(0, 2, 1)  # Permute the dimensions to change to [batch_size, 12, 1000]\n",
    "        X_val = X_val.permute(0, 2, 1)  # Permute the dimensions to change to [batch_size, 12, 1000]\n",
    "\n",
    "        # Implement the training process for your model here\n",
    "        # You should use torch.nn.Module's optimization, loss, and train loop\n",
    "        # Make sure your model is compatible with float input and target data\n",
    "\n",
    "        # Example of training loop (update with your model and data):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        # criterion = nn.BCELoss() \n",
    "        criterion = nn.BCEWithLogitsLoss() \n",
    "\n",
    "        epochs = 100\n",
    "        batch_size = 128\n",
    "\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "\n",
    "        train_losses = []  # To store training losses\n",
    "        val_losses = []    # To store validation losses\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            num_batches = len(X_train) // batch_size\n",
    "\n",
    "            num_correct = 0\n",
    "            num_samples = 0\n",
    "            train_loss = 0.0\n",
    "            val_loss = 0.0\n",
    "\n",
    "            for batch in range(num_batches):\n",
    "                start = batch * batch_size\n",
    "                end = (batch + 1) * batch_size\n",
    "                batch_X = X_train[start:end]\n",
    "                batch_y = y_train[start:end]\n",
    "                # # Check batch_y\n",
    "                # print(\"Value of batch_y:\", batch_y)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                # Calculate accuracy\n",
    "                num_correct += (self.accuracy_multi(outputs, batch_y, sigmoid=sigmoid) * batch_X.size(0)).item()\n",
    "                num_samples += batch_X.size(0)\n",
    "            train_accuracy = (num_correct / num_samples) * 100\n",
    "            train_accuracies.append(train_accuracy)\n",
    "\n",
    "            # Validation loss calculation\n",
    "            self.eval()\n",
    "            num_val_batches = len(X_val) // batch_size\n",
    "            num_correct = 0\n",
    "            num_samples = 0\n",
    "\n",
    "            for batch in range(num_val_batches):\n",
    "                start = batch * batch_size\n",
    "                end = (batch + 1) * batch_size\n",
    "                batch_X_val = X_val[start:end]\n",
    "                batch_y_val = y_val[start:end]\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs_val = self(batch_X_val) \n",
    "                    loss_val = criterion(outputs_val, batch_y_val)\n",
    "                    val_loss += loss_val.item()\n",
    "                # #Calculate accuracy\n",
    "                num_correct += (self.accuracy_multi(outputs_val, batch_y_val, sigmoid=sigmoid) * batch_X_val.size(0)).item()\n",
    "                num_samples += batch_X_val.size(0)\n",
    "            # Calculate accuracy for validation at the end of the epoch\n",
    "            val_accuracy = (num_correct / num_samples) * 100\n",
    "            val_accuracies.append(val_accuracy)\n",
    "\n",
    "            train_loss /= num_batches\n",
    "            val_loss /= num_val_batches\n",
    "            # scheduler.step(val_loss)  # Update learning rate\n",
    "\n",
    "            # Append the losses to the lists\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "\n",
    "            if model_save_path:\n",
    "                self.save_model(model_save_path)\n",
    "\n",
    "        # Plot the training and validation loss curves\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, epochs + 1), train_losses, label='Training Loss', marker='o')\n",
    "        plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss', marker='o')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Save the plot as an image\n",
    "        loss_plot_path = \"/global/D1/homes/jayao/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.2/output/custom_test/models/custom/loss_plot.png\"\n",
    "        plt.savefig(loss_plot_path)\n",
    "\n",
    "    def fine_tune(self, X_train, y_train, X_val, y_val, pretrainedfolder, mpath, sigmoid=True, model_save_path=None):\n",
    "        # Load the pre-trained model\n",
    "        pretrained_model = self.load_model(pretrainedfolder, categories=self.num_classes)\n",
    "\n",
    "        # Set the path for storing the fine-tuned model\n",
    "        self.outputfolder = mpath\n",
    "\n",
    "        # Copy the state_dict from the pre-trained model to the current model\n",
    "        self.load_state_dict(pretrained_model.state_dict())\n",
    "\n",
    "        # Define your fine-tuning process\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Create a list of layer groups for gradual unfreezing\n",
    "        layer_groups = [self.conv1, self.block1, self.block2, self.fc1, self.fc2]\n",
    "\n",
    "        # Discriminative learning rates (adjust as needed)\n",
    "        lrs = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]  # Example learning rates\n",
    "\n",
    "        # Gradual unfreezing and training loop\n",
    "        for idx, layer_group in enumerate(layer_groups):\n",
    "            if idx > 0:\n",
    "                # Unfreeze the next layer group\n",
    "                for param in layer_group.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "            # Create a new optimizer with the appropriate learning rate\n",
    "            optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=lrs[idx])\n",
    "\n",
    "            epochs = 10  # Adjust the number of epochs as needed\n",
    "\n",
    "            # Training loop for the current layer group\n",
    "            for epoch in range(epochs):\n",
    "                self.train()\n",
    "                # Implement your training loop here with the current optimizer\n",
    "                # Use criterion for loss calculation and optimizer.step() for parameter updates\n",
    "\n",
    "            # Save the model at the end of each layer group if needed\n",
    "            if model_save_path:\n",
    "                self.save_model(model_save_path)\n",
    "\n",
    "# Create an instance of YourModel\n",
    "model = YourModel(categories=5)\n",
    "\n",
    "# Fine-tune the model with discriminative learning rates and gradual unfreezing\n",
    "model.fine_tune(X_train, y_train, X_val, y_val, pretrainedfolder, mpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
