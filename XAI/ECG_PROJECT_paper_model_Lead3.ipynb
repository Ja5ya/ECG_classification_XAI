{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocess data to get training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MJx9Nhz6wOEC"
      },
      "outputs": [],
      "source": [
        "# Import package\n",
        "import time\n",
        "import numpy as np\n",
        "import wfdb\n",
        "import ast\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pylab import mpl\n",
        "from scipy.fftpack import fft, ifft \n",
        "from scipy import signal\n",
        "# from biosppy.signals import ecg\n",
        "import neurokit2 as nk\n",
        "from sklearn import *\n",
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Set the read file path\n",
        "path = '/global/D1/homes/jayao/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.2/ptbxl/'\n",
        "\n",
        "X = np.load(path + 'raw100.npy', allow_pickle=True)\n",
        "sampling_rate = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(21801, 1000, 12)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Extracting values from the third column (Lead 3)\n",
        "X2 = X[:, :, 2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the file and convert tags\n",
        "Y = pd.read_csv(path+'ptbxl_database.csv', index_col='ecg_id')\n",
        "Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(21801, 1000)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get diagnostic information in scp_statements.csv\n",
        "agg_df = pd.read_csv(path+'scp_statements.csv', index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "agg_df = agg_df[agg_df.diagnostic == 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def diagnostic_class(scp):\n",
        "    res = set()\n",
        "    for k in scp.keys():\n",
        "        if k in agg_df.index:\n",
        "            res.add(agg_df.loc[k].diagnostic_class)\n",
        "    return list(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def aggregate_diagnostic(y_dic):\n",
        "    tmp = []\n",
        "    for key in y_dic.keys():\n",
        "        if key in agg_df.index:\n",
        "            tmp.append(agg_df.loc[key].diagnostic_class)\n",
        "    return list(set(tmp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "Y['scp_classes'] = Y.scp_codes.apply(diagnostic_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NORM</th>\n",
              "      <th>MI</th>\n",
              "      <th>STTC</th>\n",
              "      <th>CD</th>\n",
              "      <th>HYP</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ecg_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21833</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21834</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21835</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21836</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21837</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>21801 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        NORM  MI  STTC  CD  HYP\n",
              "ecg_id                         \n",
              "1          1   0     0   0    0\n",
              "2          1   0     0   0    0\n",
              "3          1   0     0   0    0\n",
              "4          1   0     0   0    0\n",
              "5          1   0     0   0    0\n",
              "...      ...  ..   ...  ..  ...\n",
              "21833      0   0     1   0    0\n",
              "21834      1   0     0   0    0\n",
              "21835      0   0     1   0    0\n",
              "21836      1   0     0   0    0\n",
              "21837      1   0     0   0    0\n",
              "\n",
              "[21801 rows x 5 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Z = pd.DataFrame(0, index=Y.index, columns=['NORM', 'MI', 'STTC', 'CD', 'HYP'], dtype='int')\n",
        "for i in Z.index:\n",
        "    for k in Y.loc[i].scp_classes:\n",
        "        Z.loc[i, k] = 1\n",
        "\n",
        "Z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Add diagnostic information\n",
        "Y['diagnostic_superclass'] = Y.scp_codes.apply(aggregate_diagnostic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "diagnostic_superclass\n",
              "[NORM]                 9072\n",
              "[MI]                   2532\n",
              "[STTC]                 2401\n",
              "[CD]                   1708\n",
              "[CD, MI]               1300\n",
              "[HYP, STTC]             781\n",
              "[STTC, MI]              600\n",
              "[HYP]                   535\n",
              "[CD, STTC]              471\n",
              "[NORM, CD]              407\n",
              "[]                      405\n",
              "[HYP, STTC, MI]         361\n",
              "[HYP, CD]               300\n",
              "[CD, STTC, MI]          223\n",
              "[HYP, MI]               183\n",
              "[HYP, CD, MI]           117\n",
              "[HYP, STTC, CD]         109\n",
              "[CD, HYP, STTC, MI]      99\n",
              "[HYP, CD, STTC]          98\n",
              "[HYP, CD, STTC, MI]      54\n",
              "[NORM, STTC]             28\n",
              "[NORM, CD, STTC]          5\n",
              "[STTC, HYP, CD]           4\n",
              "[STTC, HYP, CD, MI]       3\n",
              "[NORM, HYP, CD]           2\n",
              "[NORM, HYP]               2\n",
              "[NORM, HYP, CD, MI]       1\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y.diagnostic_superclass.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('NORM',) ('MI',) () ('STTC',) ('HYP',) ('CD',) ('STTC', 'MI')\n",
            " ('HYP', 'CD') ('CD', 'MI') ('CD', 'STTC') ('HYP', 'MI')\n",
            " ('HYP', 'STTC', 'MI') ('HYP', 'CD', 'STTC', 'MI') ('HYP', 'STTC')\n",
            " ('NORM', 'CD') ('CD', 'STTC', 'MI') ('HYP', 'STTC', 'CD')\n",
            " ('CD', 'HYP', 'STTC', 'MI') ('HYP', 'CD', 'STTC')\n",
            " ('STTC', 'HYP', 'CD', 'MI') ('NORM', 'STTC') ('HYP', 'CD', 'MI')\n",
            " ('NORM', 'CD', 'STTC') ('NORM', 'HYP', 'CD') ('NORM', 'HYP')\n",
            " ('STTC', 'HYP', 'CD') ('NORM', 'HYP', 'CD', 'MI')]\n"
          ]
        }
      ],
      "source": [
        "unique_values = Y['diagnostic_superclass'].apply(tuple).unique()\n",
        "print(unique_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(17420, 1000) (17420,)\n",
            "(4381, 1000) (4381,)\n"
          ]
        }
      ],
      "source": [
        "# Split data into train and test\n",
        "test_fold = 10\n",
        "# # Train\n",
        "X_train = X2[(Y.strat_fold <= 8)]\n",
        "# y_train = Z[Y.strat_fold <= 8]\n",
        "y_train = Y[(Y.strat_fold <= 8)].diagnostic_superclass\n",
        "# # Test\n",
        "X_test = X2[(Y.strat_fold >8)]\n",
        "# y_test = Z[Y.strat_fold > 8]\n",
        "y_test = Y[(Y.strat_fold > 8)].diagnostic_superclass\n",
        "\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape,  y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_path = '/global/D1/homes/jayao/XAI-Based-ECG-Diagnostics-main/data/lead3/'\n",
        "\n",
        "np.save(save_path+'x_train.npy', X_train)\n",
        "np.save(save_path+'y_train.npy', np.array(y_train))\n",
        "np.save(save_path+'x_test.npy', X_test)\n",
        "np.save(save_path+'y_test.npy', np.array(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ECG SHaP starts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-21 15:52:22.788005: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-21 15:52:22.788055: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-21 15:52:22.789021: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-21 15:52:22.869892: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-02-21 15:52:25.495733: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers, optimizers, losses, metrics, activations, regularizers, callbacks\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJxT5hv-wjBM",
        "outputId": "dd2d52f3-a3c2-4c46-9eea-9ec6046ac22f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(17420, 1000)\n",
            "(4381, 1000)\n"
          ]
        }
      ],
      "source": [
        "path = \"/global/D1/homes/jayao/XAI-Based-ECG-Diagnostics-main/data/lead3/\"\n",
        "x_train = np.load(path + 'x_train.npy')\n",
        "y_train = np.load(path + 'y_train.npy', allow_pickle=True)\n",
        "x_test  = np.load(path + 'x_test.npy')\n",
        "y_test  = np.load(path + 'y_test.npy', allow_pickle=True)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(17420, 1000, 1)\n",
            "(4381, 1000, 1)\n"
          ]
        }
      ],
      "source": [
        "x_train = x_train.reshape((17420, 1000, 1))\n",
        "x_test = x_test.reshape((4381, 1000, 1))\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(17420, 1, 1000)\n",
            "(4381, 1, 1000)\n"
          ]
        }
      ],
      "source": [
        "x_train = x_train.transpose(0, 2, 1)            # transpose working correctly\n",
        "x_test  = x_test.transpose(0, 2, 1)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train = x_train.reshape(17420, 1, 1000, 1)   # Add another channel\n",
        "x_test  = x_test.reshape(4381, 1, 1000, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train : (17420, 1, 1000, 1)\n",
            "y_train : (17420,)\n",
            "x_test  : (4381, 1, 1000, 1)\n",
            "y_test  : (4381,)\n",
            "Data loaded\n"
          ]
        }
      ],
      "source": [
        "print(\"x_train :\", x_train.shape)\n",
        "print(\"y_train :\", y_train.shape)\n",
        "print(\"x_test  :\", x_test.shape)\n",
        "print(\"y_test  :\", y_test.shape)\n",
        "print('Data loaded')\n",
        "\n",
        "# Old OUTPUTS:\n",
        "# (19601, 1000, 12)\n",
        "# (19601, 12, 1000)\n",
        "# x_train : (19601, 12, 1000, 1)\n",
        "# y_train : (19601,)\n",
        "# x_test  : (2198, 12, 1000, 1)\n",
        "# y_test  : (2198,)\n",
        "# Data loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9szwf9I40VVc"
      },
      "outputs": [],
      "source": [
        "# x_train = x_train[:2000]\n",
        "# x_test = x_test[:500]\n",
        "# y_train = y_train[:2000]\n",
        "# y_test = y_test[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxCCW4SO0fuX",
        "outputId": "3ad1f11c-835e-499b-ae90-d6b9c9a8c9f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4381, 1, 1000, 1)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffvWI7S-KWPh",
        "outputId": "6e1db6ca-be32-4992-e7f0-3c1a674e878b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classes: ['CD' 'HYP' 'MI' 'NORM' 'STTC']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "# Convert multi-label target labels to one-hot encoded matrix\n",
        "mlb = MultiLabelBinarizer()\n",
        "y_train = mlb.fit_transform(y_train)\n",
        "y_test = mlb.transform(y_test)\n",
        "print(\"Classes:\", mlb.classes_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk3y3dB3x65D",
        "outputId": "476fc2a0-a68b-4785-c210-f6ac3706e5bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-21 15:52:31.804288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1457 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:5e:00.0, compute capability: 7.5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added 5 layers for temporal analysis\n",
            "Added 1 layer for spatial Analysis\n",
            "Added 2 fully connected layers\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 1, 1000, 1)]         0         []                            \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)             (None, 1, 1000, 32)          192       ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization (Batch  (None, 1, 1000, 32)          128       ['conv2d[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " re_lu (ReLU)                (None, 1, 1000, 32)          0         ['batch_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2  (None, 1, 1000, 32)          0         ['re_lu[0][0]']               \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)           (None, 1, 1000, 32)          5152      ['max_pooling2d[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_1 (Bat  (None, 1, 1000, 32)          128       ['conv2d_2[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " re_lu_1 (ReLU)              (None, 1, 1000, 32)          0         ['batch_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPoolin  (None, 1, 1000, 32)          0         ['re_lu_1[0][0]']             \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)           (None, 1, 1000, 64)          10304     ['max_pooling2d_1[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)           (None, 1, 1000, 64)          14400     ['max_pooling2d[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_2 (Bat  (None, 1, 1000, 64)          256       ['conv2d_4[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)           (None, 1, 1000, 64)          24640     ['conv2d_1[0][0]']            \n",
            "                                                                                                  \n",
            " re_lu_2 (ReLU)              (None, 1, 1000, 64)          0         ['batch_normalization_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add (Add)                   (None, 1, 1000, 64)          0         ['conv2d_3[0][0]',            \n",
            "                                                                     're_lu_2[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_3 (ReLU)              (None, 1, 1000, 64)          0         ['add[0][0]']                 \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPoolin  (None, 1, 1000, 64)          0         ['re_lu_3[0][0]']             \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)           (None, 1, 1000, 64)          12352     ['max_pooling2d_2[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_3 (Bat  (None, 1, 1000, 64)          256       ['conv2d_6[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " re_lu_4 (ReLU)              (None, 1, 1000, 64)          0         ['batch_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPoolin  (None, 1, 1000, 64)          0         ['re_lu_4[0][0]']             \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)           (None, 1, 1000, 64)          12352     ['max_pooling2d_3[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)           (None, 1, 1000, 32)          8224      ['max_pooling2d_2[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_4 (Bat  (None, 1, 1000, 64)          256       ['conv2d_8[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)           (None, 1, 1000, 64)          10304     ['conv2d_5[0][0]']            \n",
            "                                                                                                  \n",
            " re_lu_5 (ReLU)              (None, 1, 1000, 64)          0         ['batch_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 1, 1000, 64)          0         ['conv2d_7[0][0]',            \n",
            "                                                                     're_lu_5[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_6 (ReLU)              (None, 1, 1000, 64)          0         ['add_1[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling2d_4 (MaxPoolin  (None, 1, 1000, 64)          0         ['re_lu_6[0][0]']             \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)           (None, 1, 1000, 64)          49216     ['max_pooling2d_4[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_5 (Bat  (None, 1, 1000, 64)          256       ['conv2d_9[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " re_lu_7 (ReLU)              (None, 1, 1000, 64)          0         ['batch_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " global_average_pooling2d (  (None, 64)                   0         ['re_lu_7[0][0]']             \n",
            " GlobalAveragePooling2D)                                                                          \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 64)                   0         ['global_average_pooling2d[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 128)                  8320      ['flatten[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_6 (Bat  (None, 128)                  512       ['dense[0][0]']               \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " re_lu_8 (ReLU)              (None, 128)                  0         ['batch_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 128)                  0         ['re_lu_8[0][0]']             \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 64)                   8256      ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_7 (Bat  (None, 64)                   256       ['dense_1[0][0]']             \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " re_lu_9 (ReLU)              (None, 64)                   0         ['batch_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 64)                   0         ['re_lu_9[0][0]']             \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 5)                    325       ['dropout_1[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 166085 (648.77 KB)\n",
            "Trainable params: 165061 (644.77 KB)\n",
            "Non-trainable params: 1024 (4.00 KB)\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Main Version\n",
        "input = layers.Input(shape=(1, 1000, 1))\n",
        "\n",
        "X = layers.Conv2D(filters=32, kernel_size=(1, 5), padding='same')(input)\n",
        "X = layers.BatchNormalization()(X)\n",
        "X = layers.ReLU()(X)\n",
        "X = layers.MaxPooling2D(pool_size=(1, 2), strides=1, padding='same')(X)\n",
        "\n",
        "convC1 = layers.Conv2D(filters=64, kernel_size=(1, 7), padding='same')(X)\n",
        "\n",
        "X = layers.Conv2D(filters=32, kernel_size=(1, 5), padding='same')(X)\n",
        "X = layers.BatchNormalization()(X)\n",
        "X = layers.ReLU()(X)\n",
        "X = layers.MaxPooling2D(pool_size=(1, 4), strides=1, padding='same')(X)\n",
        "\n",
        "convC2 = layers.Conv2D(filters=64, kernel_size=(1, 6), padding='same')(convC1)\n",
        "\n",
        "X = layers.Conv2D(filters=64, kernel_size=(1, 5), padding='same')(X)\n",
        "X = layers.BatchNormalization()(X)\n",
        "X = layers.ReLU()(X)\n",
        "residual_1 = layers.Add()([convC2, X])           # skip Connection\n",
        "X = layers.ReLU()(residual_1)\n",
        "X = layers.MaxPooling2D(pool_size=(1, 2), strides=1, padding='same')(X)\n",
        "\n",
        "convE1 = layers.Conv2D(filters=32, kernel_size=(1, 4), padding='same')(X)\n",
        "\n",
        "X = layers.Conv2D(filters=64, kernel_size=(1, 3), padding='same')(X)\n",
        "X = layers.BatchNormalization()(X)\n",
        "X = layers.ReLU()(X)\n",
        "X = layers.MaxPooling2D(pool_size=(1, 4), strides=1, padding='same')(X)\n",
        "\n",
        "convE2 = layers.Conv2D(filters=64, kernel_size=(1, 5), padding='same')(convE1)\n",
        "\n",
        "X = layers.Conv2D(filters=64, kernel_size=(1, 3), padding='same')(X)\n",
        "X = layers.BatchNormalization()(X)\n",
        "X = layers.ReLU()(X)\n",
        "residual_2 = layers.Add()([convE2, X])         # skip Connection\n",
        "X = layers.ReLU()(residual_2)\n",
        "X = layers.MaxPooling2D(pool_size=(1, 2), strides=1, padding='same')(X)\n",
        "print('Added 5 layers for temporal analysis')\n",
        "\n",
        "# Spatial Analysis\n",
        "X = layers.Conv2D(filters=64, kernel_size=(12, 1), padding='same')(X)\n",
        "X = layers.BatchNormalization()(X)\n",
        "X = layers.ReLU()(X)\n",
        "X = layers.GlobalAveragePooling2D()(X)\n",
        "print('Added 1 layer for spatial Analysis')\n",
        "\n",
        "# Fully Connected Layers\n",
        "X = layers.Flatten()(X)\n",
        "X = layers.Dense(units=128, kernel_regularizer=regularizers.L2(0.005))(X)\n",
        "X = layers.BatchNormalization()(X)\n",
        "X = layers.ReLU()(X)\n",
        "X = layers.Dropout(rate=0.3)(X)\n",
        "\n",
        "X = layers.Dense(units=64, kernel_regularizer=regularizers.L2(0.009))(X)\n",
        "X = layers.BatchNormalization()(X)\n",
        "X = layers.ReLU()(X)\n",
        "X = layers.Dropout(rate=0.3)(X)\n",
        "print('Added 2 fully connected layers')\n",
        "\n",
        "# Output Layer\n",
        "output = layers.Dense(5, activation='sigmoid')(X)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=input, outputs=output)\n",
        "print(model.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ts0oUWQe98rm",
        "outputId": "d40dd14f-9d6d-4798-99d4-1344474b4485"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-21 15:52:36.762976: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8600\n",
            "2024-02-21 15:52:36.966389: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
            "2024-02-21 15:52:37.147297: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
            "2024-02-21 15:52:37.458201: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.05GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2024-02-21 15:52:39.970386: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1551340b8870 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2024-02-21 15:52:39.970430: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2024-02-21 15:52:39.978217: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2024-02-21 15:52:40.075977: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "2024-02-21 15:52:40.922904: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.05GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "240/240 [==============================] - 22s 43ms/step - loss: 1.0406 - binary_accuracy: 0.7505 - auc: 0.6476 - val_loss: 0.7247 - val_binary_accuracy: 0.7463 - val_auc: 0.6330 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "240/240 [==============================] - 9s 38ms/step - loss: 0.5626 - binary_accuracy: 0.7885 - auc: 0.7061 - val_loss: 0.6238 - val_binary_accuracy: 0.7022 - val_auc: 0.6931 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "240/240 [==============================] - 9s 38ms/step - loss: 0.4948 - binary_accuracy: 0.7954 - auc: 0.7249 - val_loss: 0.5271 - val_binary_accuracy: 0.7637 - val_auc: 0.7144 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "240/240 [==============================] - 9s 38ms/step - loss: 0.4718 - binary_accuracy: 0.7993 - auc: 0.7423 - val_loss: 0.5136 - val_binary_accuracy: 0.7645 - val_auc: 0.7377 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "240/240 [==============================] - 9s 38ms/step - loss: 0.4663 - binary_accuracy: 0.8022 - auc: 0.7455 - val_loss: 0.4875 - val_binary_accuracy: 0.7815 - val_auc: 0.7535 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "240/240 [==============================] - 9s 38ms/step - loss: 0.4594 - binary_accuracy: 0.8051 - auc: 0.7537 - val_loss: 0.4957 - val_binary_accuracy: 0.7812 - val_auc: 0.7502 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "240/240 [==============================] - 9s 39ms/step - loss: 0.4539 - binary_accuracy: 0.8066 - auc: 0.7626 - val_loss: 0.5866 - val_binary_accuracy: 0.7659 - val_auc: 0.7188 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "240/240 [==============================] - 9s 39ms/step - loss: 0.4523 - binary_accuracy: 0.8065 - auc: 0.7647 - val_loss: 0.5025 - val_binary_accuracy: 0.7734 - val_auc: 0.7473 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "240/240 [==============================] - 9s 39ms/step - loss: 0.4352 - binary_accuracy: 0.8154 - auc: 0.7846 - val_loss: 0.4605 - val_binary_accuracy: 0.7954 - val_auc: 0.7866 - lr: 1.0000e-04\n",
            "Epoch 10/20\n",
            "240/240 [==============================] - 9s 39ms/step - loss: 0.4264 - binary_accuracy: 0.8173 - auc: 0.7948 - val_loss: 0.4405 - val_binary_accuracy: 0.8048 - val_auc: 0.7947 - lr: 1.0000e-04\n",
            "Epoch 11/20\n",
            "240/240 [==============================] - 9s 39ms/step - loss: 0.4220 - binary_accuracy: 0.8197 - auc: 0.7984 - val_loss: 0.4449 - val_binary_accuracy: 0.8026 - val_auc: 0.7937 - lr: 1.0000e-04\n",
            "Epoch 12/20\n",
            "240/240 [==============================] - 9s 39ms/step - loss: 0.4182 - binary_accuracy: 0.8207 - auc: 0.8014 - val_loss: 0.4420 - val_binary_accuracy: 0.8045 - val_auc: 0.7970 - lr: 1.0000e-04\n",
            "Epoch 13/20\n",
            "240/240 [==============================] - 9s 39ms/step - loss: 0.4179 - binary_accuracy: 0.8214 - auc: 0.8009 - val_loss: 0.4385 - val_binary_accuracy: 0.8058 - val_auc: 0.8005 - lr: 1.0000e-04\n",
            "Epoch 14/20\n",
            "240/240 [==============================] - 9s 39ms/step - loss: 0.4150 - binary_accuracy: 0.8213 - auc: 0.8035 - val_loss: 0.4326 - val_binary_accuracy: 0.8081 - val_auc: 0.8001 - lr: 1.0000e-04\n",
            "Epoch 15/20\n",
            "240/240 [==============================] - 9s 39ms/step - loss: 0.4130 - binary_accuracy: 0.8228 - auc: 0.8062 - val_loss: 0.4384 - val_binary_accuracy: 0.8025 - val_auc: 0.7994 - lr: 1.0000e-04\n",
            "Epoch 16/20\n",
            "240/240 [==============================] - 9s 39ms/step - loss: 0.4116 - binary_accuracy: 0.8226 - auc: 0.8080 - val_loss: 0.4332 - val_binary_accuracy: 0.8056 - val_auc: 0.8029 - lr: 1.0000e-04\n",
            "Epoch 17/20\n",
            "240/240 [==============================] - 9s 39ms/step - loss: 0.4098 - binary_accuracy: 0.8241 - auc: 0.8089 - val_loss: 0.4353 - val_binary_accuracy: 0.8055 - val_auc: 0.8006 - lr: 1.0000e-04\n",
            "Epoch 18/20\n",
            "240/240 [==============================] - 9s 39ms/step - loss: 0.4061 - binary_accuracy: 0.8257 - auc: 0.8136 - val_loss: 0.4242 - val_binary_accuracy: 0.8098 - val_auc: 0.8063 - lr: 1.0000e-05\n",
            "Epoch 19/20\n",
            "240/240 [==============================] - 9s 39ms/step - loss: 0.4052 - binary_accuracy: 0.8258 - auc: 0.8143 - val_loss: 0.4236 - val_binary_accuracy: 0.8092 - val_auc: 0.8068 - lr: 1.0000e-05\n",
            "Epoch 20/20\n",
            "240/240 [==============================] - 9s 39ms/step - loss: 0.4040 - binary_accuracy: 0.8272 - auc: 0.8156 - val_loss: 0.4255 - val_binary_accuracy: 0.8089 - val_auc: 0.8059 - lr: 1.0000e-05\n"
          ]
        }
      ],
      "source": [
        "\n",
        "early    = callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True)\n",
        "reducelr = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=3)\n",
        "\n",
        "callback = [early, reducelr]\n",
        "\n",
        "model.compile(optimizer = optimizers.Adam(learning_rate=0.001),\n",
        "              loss = losses.BinaryCrossentropy(),\n",
        "              metrics = [metrics.BinaryAccuracy(), metrics.AUC(curve='ROC', multi_label=True)])\n",
        "\n",
        "history = model.fit(x_train, y_train, validation_split=0.12, epochs=20, batch_size=64, callbacks=callback)\n",
        "# history = model.fit(x_train, y_train, validation_split=0.10, epochs=25, batch_size=64, callbacks=callback)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V65iMb0hGJNE"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LKWbmNBuGJkX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/global/D1/homes/jayao/.venv/lib/python3.11/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "save_path = \"/global/D1/homes/jayao/XAI-Based-ECG-Diagnostics-main/model/\"\n",
        "model.save(save_path + \"ST-CNN-5_lead3new.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "137/137 [==============================] - 1s 7ms/step\n",
            "Sample 1: Predicted Classes - ()\n",
            "Sample 2: Predicted Classes - ('NORM',)\n",
            "Sample 3: Predicted Classes - ('NORM',)\n",
            "Sample 4: Predicted Classes - ('CD',)\n",
            "Sample 5: Predicted Classes - ()\n"
          ]
        }
      ],
      "source": [
        "# Assuming you have a trained model 'model'\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# Assuming y_pred is in the probability format (values between 0 and 1)\n",
        "# Convert the probabilities to binary predictions using a threshold (e.g., 0.5)\n",
        "y_pred_binary = (y_pred >= 0.5).astype(int)\n",
        "\n",
        "# Convert the binary predictions back to class labels using MultiLabelBinarizer\n",
        "predicted_classes = mlb.inverse_transform(y_pred_binary)\n",
        "\n",
        "# Display the predicted classes for the first few samples\n",
        "for i in range(5):\n",
        "    print(f\"Sample {i + 1}: Predicted Classes - {predicted_classes[i]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "137/137 [==============================] - 1s 6ms/step\n",
            "All Distinct Classes: {'HYP', 'CD', 'NORM', 'STTC', 'MI'}\n"
          ]
        }
      ],
      "source": [
        "# Assuming you have a trained model 'model'\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# Assuming y_pred is in the probability format (values between 0 and 1)\n",
        "# Convert the probabilities to binary predictions using a threshold (e.g., 0.5)\n",
        "y_pred_binary = (y_pred >= 0.5).astype(int)\n",
        "\n",
        "# Convert the binary predictions back to class labels using MultiLabelBinarizer\n",
        "predicted_classes = mlb.inverse_transform(y_pred_binary)\n",
        "\n",
        "# Use a set to accumulate all distinct classes\n",
        "distinct_classes = set()\n",
        "\n",
        "# Iterate over predicted classes and add them to the set\n",
        "for classes in predicted_classes:\n",
        "    distinct_classes.update(classes)\n",
        "\n",
        "# Display all distinct classes\n",
        "print(\"All Distinct Classes:\", distinct_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwzwSVs1Q1Gs",
        "outputId": "5991f287-3be4-4ba4-d3ed-765e0ca02143"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "545/545 [==============================] - 4s 6ms/step\n",
            "137/137 [==============================] - 1s 6ms/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model = load_model(r'/global/D1/homes/jayao/XAI-Based-ECG-Diagnostics-main/model/ST-CNN-5_lead3new.h5')\n",
        "y_pred_train = model.predict(x_train)\n",
        "y_pred_test  = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy        : 81.10\n",
            "Macro AUC score : 79.11\n",
            "AUROC           : 57.02\n",
            "Micro F1 score  : 56.02\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          CD       0.79      0.44      0.57       992\n",
            "         HYP       0.64      0.03      0.05       530\n",
            "          MI       0.70      0.40      0.51      1092\n",
            "        NORM       0.68      0.83      0.75      1919\n",
            "        STTC       0.56      0.15      0.23      1049\n",
            "\n",
            "   micro avg       0.69      0.47      0.56      5582\n",
            "   macro avg       0.67      0.37      0.42      5582\n",
            "weighted avg       0.68      0.47      0.51      5582\n",
            " samples avg       0.56      0.51      0.52      5582\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/global/D1/homes/jayao/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/global/D1/homes/jayao/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, precision_recall_curve, f1_score, roc_auc_score, accuracy_score, auc\n",
        "import numpy as np\n",
        "\n",
        "def sklearn_metrics(y_true, y_pred, mlb):\n",
        "    y_bin = np.copy(y_pred)\n",
        "    y_bin[y_bin >= 0.5] = 1\n",
        "    y_bin[y_bin < 0.5]  = 0\n",
        "\n",
        "    # Compute area under precision-Recall curve\n",
        "    auc_sum = 0\n",
        "    for i in range(y_true.shape[1]):\n",
        "        precision, recall, thresholds = precision_recall_curve(y_true[:, i], y_pred[:, i])\n",
        "        auc_sum += auc(recall, precision)\n",
        "\n",
        "    print(\"Accuracy        : {:.2f}\".format(accuracy_score(y_true.flatten(), y_bin.flatten()) * 100))\n",
        "    print(\"Macro AUC score : {:.2f}\".format(roc_auc_score(y_true, y_pred, average='macro') * 100))\n",
        "    print('AUROC           : {:.2f}'.format((auc_sum / y_true.shape[1]) * 100))\n",
        "    print(\"Micro F1 score  : {:.2f}\".format(f1_score(y_true, y_bin, average='micro') * 100))\n",
        "\n",
        "    # Convert binary predictions back to class labels using MultiLabelBinarizer\n",
        "    predicted_classes = mlb.inverse_transform(y_bin)\n",
        "\n",
        "    # Use a set to accumulate all distinct classes\n",
        "    distinct_classes = set()\n",
        "\n",
        "    # Iterate over predicted classes and add them to the set\n",
        "    for classes in predicted_classes:\n",
        "        distinct_classes.update(classes)\n",
        "\n",
        "    # Convert the set of distinct classes to a sorted list\n",
        "    class_names = sorted(list(distinct_classes))\n",
        "\n",
        "    # Print classification report for each class\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_bin, target_names=class_names))\n",
        "\n",
        "# Assuming mlb is the MultiLabelBinarizer used for transforming the labels\n",
        "sklearn_metrics(y_test, y_pred_test, mlb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Accuracy        : 80.38\n",
        "# Macro AUC score : 76.58\n",
        "# AUROC           : 53.55\n",
        "# Micro F1 score  : 51.97\n",
        "\n",
        "# Classification Report:\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#           CD       0.79      0.36      0.49       992\n",
        "#          HYP       0.50      0.00      0.01       530\n",
        "#           MI       0.73      0.34      0.46      1092\n",
        "#         NORM       0.67      0.80      0.73      1919\n",
        "#         STTC       0.54      0.06      0.11      1049\n",
        "\n",
        "#    micro avg       0.69      0.42      0.52      5582\n",
        "#    macro avg       0.65      0.31      0.36      5582\n",
        "# weighted avg       0.66      0.42      0.45      5582\n",
        "#  samples avg       0.51      0.46      0.47      5582"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
